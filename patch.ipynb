{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranavgovi/dementia-detection-/blob/main/patch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnQZBP5RzVAR",
        "outputId": "7ba50d45-be30-42cf-bf54-441e2c6a0166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting patchify\n",
            "  Downloading patchify-0.2.3-py3-none-any.whl (6.6 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.7/dist-packages (from patchify) (1.21.6)\n",
            "Installing collected packages: patchify\n",
            "Successfully installed patchify-0.2.3\n"
          ]
        }
      ],
      "source": [
        "pip install patchify"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "projection_dim=64"
      ],
      "metadata": {
        "id": "rceAXW82nzqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsvU40NNUVm0"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from numpy import asarray\n",
        "from patchify import patchify\n",
        "import cv2\n",
        "import numpy as np\n",
        "from patchify import patchify\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qk45jau8UjeJ",
        "outputId": "6175e9e3-df27-438d-b329-e4466ca79fad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwGNFzIF6Wgb"
      },
      "outputs": [],
      "source": [
        "num_classes = 2\n",
        "input_shape = (280,280,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQra0BQtyugk"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "Normal = glob.glob('/content/drive/MyDrive/Pitt/Cookie/spectogram/Normal_spec/*.*')\n",
        "Dimentia = glob.glob('/content/drive/MyDrive/Pitt/Cookie/spectogram/dimentia_spec/*.*')\n",
        "\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "for i in Normal:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n",
        "    target_size= (280,280))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append(0)\n",
        "for i in Dimentia:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n",
        "    target_size= (280,280))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append(1)\n",
        "\n",
        "\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, ytrain, ytest = train_test_split(data, labels, test_size=0.2,\n",
        "                                                random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzRLByhM0jjX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "09917c66-3366-4faa-e117-a3f3f8fd1f3f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-68cea0bafeb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"
          ]
        }
      ],
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO_DO4YQytCE"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.0020\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 50\n",
        "image_size = 72  # We'll resize input images to this size\n",
        "patch_size = 6  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim ,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipBFj15-2Ka3"
      },
      "outputs": [],
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-okWhOP0Ps-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "ab2de4ad-37c6-46b0-be9f-6cece18b1317"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-5224ce192702>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m data_augmentation = keras.Sequential(\n\u001b[1;32m      7\u001b[0m     [\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_addons'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(72, 72),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "data_augmentation.layers[0].adapt(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iB01BlZ187p"
      },
      "outputs": [],
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        " \n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxZMmGFT3oiY"
      },
      "outputs": [],
      "source": [
        "def create_vit_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wd7GKpWPq2mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qvt8Ggu3zXb",
        "outputId": "b0b56dff-9aeb-462a-d702-bb92d5df1810"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 28s 28s/step - loss: 2.2520 - accuracy: 0.4700 - top-5-accuracy: 1.0000 - val_loss: 65.3624 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 60.9673 - accuracy: 0.8000 - top-5-accuracy: 1.0000 - val_loss: 2.5033 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 2.6930 - accuracy: 0.8000 - top-5-accuracy: 1.0000 - val_loss: 5.7976 - val_accuracy: 0.3333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 7.2784 - accuracy: 0.7100 - top-5-accuracy: 1.0000 - val_loss: 34.0538 - val_accuracy: 0.1667 - val_top-5-accuracy: 1.0000\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 34.5771 - accuracy: 0.2000 - top-5-accuracy: 1.0000 - val_loss: 4.9769 - val_accuracy: 0.1667 - val_top-5-accuracy: 1.0000\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 3.8772 - accuracy: 0.3300 - top-5-accuracy: 1.0000 - val_loss: 6.9131 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 9.9782 - accuracy: 0.8000 - top-5-accuracy: 1.0000 - val_loss: 7.0335 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 10.8694 - accuracy: 0.8000 - top-5-accuracy: 1.0000 - val_loss: 3.5751 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 6.0405 - accuracy: 0.8000 - top-5-accuracy: 1.0000 - val_loss: 1.0121 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 2.4214 - accuracy: 0.8000 - top-5-accuracy: 1.0000 - val_loss: 2.9433 - val_accuracy: 0.1667 - val_top-5-accuracy: 1.0000\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 2.9028 - accuracy: 0.3300 - top-5-accuracy: 1.0000 - val_loss: 0.7014 - val_accuracy: 0.4167 - val_top-5-accuracy: 1.0000\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 1.4625 - accuracy: 0.5900 - top-5-accuracy: 1.0000 - val_loss: 0.6917 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 7s 7s/step - loss: 1.1099 - accuracy: 0.7900 - top-5-accuracy: 1.0000 - val_loss: 0.9473 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 1.3518 - accuracy: 0.7800 - top-5-accuracy: 1.0000 - val_loss: 0.9351 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 1.3133 - accuracy: 0.7900 - top-5-accuracy: 1.0000 - val_loss: 0.7630 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.9521 - accuracy: 0.7800 - top-5-accuracy: 1.0000 - val_loss: 0.5710 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 1.0393 - accuracy: 0.7100 - top-5-accuracy: 1.0000 - val_loss: 0.4559 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 1.0052 - accuracy: 0.6400 - top-5-accuracy: 1.0000 - val_loss: 0.4516 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.8261 - accuracy: 0.6700 - top-5-accuracy: 1.0000 - val_loss: 0.4521 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.8129 - accuracy: 0.5900 - top-5-accuracy: 1.0000 - val_loss: 0.4512 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.7078 - accuracy: 0.6600 - top-5-accuracy: 1.0000 - val_loss: 0.4508 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5545 - accuracy: 0.7700 - top-5-accuracy: 1.0000 - val_loss: 0.4501 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5699 - accuracy: 0.7800 - top-5-accuracy: 1.0000 - val_loss: 0.4511 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7211 - accuracy: 0.7900 - top-5-accuracy: 1.0000 - val_loss: 0.4550 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6343 - accuracy: 0.7500 - top-5-accuracy: 1.0000 - val_loss: 0.4716 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6088 - accuracy: 0.7200 - top-5-accuracy: 1.0000 - val_loss: 0.4942 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5590 - accuracy: 0.7800 - top-5-accuracy: 1.0000 - val_loss: 0.5182 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4778 - accuracy: 0.8000 - top-5-accuracy: 1.0000 - val_loss: 0.5274 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5658 - accuracy: 0.7700 - top-5-accuracy: 1.0000 - val_loss: 0.5173 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6008 - accuracy: 0.7400 - top-5-accuracy: 1.0000 - val_loss: 0.5002 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6414 - accuracy: 0.7000 - top-5-accuracy: 1.0000 - val_loss: 0.4894 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5499 - accuracy: 0.7900 - top-5-accuracy: 1.0000 - val_loss: 0.4826 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4911 - accuracy: 0.7900 - top-5-accuracy: 1.0000 - val_loss: 0.4815 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6008 - accuracy: 0.8100 - top-5-accuracy: 1.0000 - val_loss: 0.4923 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5513 - accuracy: 0.8000 - top-5-accuracy: 1.0000 - val_loss: 0.5138 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5532 - accuracy: 0.7700 - top-5-accuracy: 1.0000 - val_loss: 0.5303 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5222 - accuracy: 0.7900 - top-5-accuracy: 1.0000 - val_loss: 0.5352 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5337 - accuracy: 0.7800 - top-5-accuracy: 1.0000 - val_loss: 0.5461 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5593 - accuracy: 0.7700 - top-5-accuracy: 1.0000 - val_loss: 0.5512 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5586 - accuracy: 0.8100 - top-5-accuracy: 1.0000 - val_loss: 0.5512 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5489 - accuracy: 0.7900 - top-5-accuracy: 1.0000 - val_loss: 0.5423 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5108 - accuracy: 0.8200 - top-5-accuracy: 1.0000 - val_loss: 0.5236 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5867 - accuracy: 0.7600 - top-5-accuracy: 1.0000 - val_loss: 0.5058 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5041 - accuracy: 0.8000 - top-5-accuracy: 1.0000 - val_loss: 0.4885 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5809 - accuracy: 0.8000 - top-5-accuracy: 1.0000 - val_loss: 0.4800 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5563 - accuracy: 0.8100 - top-5-accuracy: 1.0000 - val_loss: 0.4812 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5350 - accuracy: 0.8000 - top-5-accuracy: 1.0000 - val_loss: 0.4903 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.5908 - accuracy: 0.7700 - top-5-accuracy: 1.0000 - val_loss: 0.5078 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.4820 - accuracy: 0.8000 - top-5-accuracy: 1.0000 - val_loss: 0.5217 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5473 - accuracy: 0.8000 - top-5-accuracy: 1.0000 - val_loss: 0.5308 - val_accuracy: 0.8333 - val_top-5-accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "      \n",
        "        ],\n",
        "    )\n",
        "\n",
        "    ##checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    ##checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        ##checkpoint_filepath,\n",
        "        ##monitor=\"val_accuracy\",\n",
        "        ##save_best_only=True,\n",
        "        ##save_weights_only=True,\n",
        "    #)\n",
        "\n",
        "    history = model.fit(\n",
        "    x=X_train,\n",
        "    y=ytrain,\n",
        "    batch_size=batch_size,\n",
        "    epochs=num_epochs,\n",
        "    validation_split=0.1,)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vit_classifier = create_vit_classifier()\n",
        "history = run_experiment(vit_classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C0-vSbtu4CRz",
        "outputId": "cccc707d-3b85-4079-e248-f89ff3d3b702"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 610ms/step - loss: 0.5209 - accuracy: 0.8571 - top-5-accuracy: 1.0000\n",
            "Test accuracy: 85.71%\n",
            "Test top 5 accuracy: 100.0%\n"
          ]
        }
      ],
      "source": [
        " _, accuracy, top_5_accuracy = vit_classifier.evaluate(X_test, ytest)\n",
        "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddztIAXc1xbP",
        "outputId": "6db0577c-a737-4563-dde1-497237d8d7a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 27.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.18.0\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wWeKkMfCzxAn",
        "outputId": "89696bb2-6baf-4e1e-e5eb-7ade4af822fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8571428571428571\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "p_pred = vit_classifier.predict(X_test)\n",
        "# [1. 0.01 0.91 0.87 0.06 0.95 0.24 0.58 0.78 ...\n",
        "\n",
        "# extract the predicted class labels\n",
        "import numpy as np\n",
        "rounded_labels=np.argmax(p_pred, axis=1)\n",
        "print(accuracy_score(ytest, rounded_labels))\n",
        "\n",
        "# [1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 ...\n",
        "print(confusion_matrix(ytest, y_pred))\n",
        "# [[13  1]\n",
        "#  [ 2  9]]\n",
        "\n",
        "##(classification_report(ytest, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E5H78m9Tddzy",
        "outputId": "36c2c3df-33ca-4fc6-f963-5d0f1b92b583"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.923076923076923"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "metrics.f1_score(ytest, rounded_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UjtP06yjSOLg",
        "outputId": "11aa5c47-37b9-4a66-d130-1ba749541b24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "metrics.recall_score(ytest,rounded_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EHcXyrLvStYy",
        "outputId": "aa4c7003-4324-4381-c9fe-f0fefa3fc8c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8571428571428571"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "metrics.precision_score(ytest,rounded_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pE56dxu6NWbn",
        "outputId": "4b6170dd-231a-46d1-d0e4-39cc88581c88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 280, 280, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " data_augmentation (Sequential)  (None, 72, 72, 3)   7           ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " patches_1 (Patches)            (None, None, 108)    0           ['data_augmentation[0][0]']      \n",
            "                                                                                                  \n",
            " patch_encoder_1 (PatchEncoder)  (None, 144, 64)     16192       ['patches_1[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_17 (LayerN  (None, 144, 64)     128         ['patch_encoder_1[0][0]']        \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_8 (MultiH  (None, 144, 64)     66368       ['layer_normalization_17[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " add_16 (Add)                   (None, 144, 64)      0           ['multi_head_attention_8[0][0]', \n",
            "                                                                  'patch_encoder_1[0][0]']        \n",
            "                                                                                                  \n",
            " layer_normalization_18 (LayerN  (None, 144, 64)     128         ['add_16[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_21 (Dense)               (None, 144, 128)     8320        ['layer_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 144, 128)     0           ['dense_21[0][0]']               \n",
            "                                                                                                  \n",
            " dense_22 (Dense)               (None, 144, 64)      8256        ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_20 (Dropout)           (None, 144, 64)      0           ['dense_22[0][0]']               \n",
            "                                                                                                  \n",
            " add_17 (Add)                   (None, 144, 64)      0           ['dropout_20[0][0]',             \n",
            "                                                                  'add_16[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_19 (LayerN  (None, 144, 64)     128         ['add_17[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_9 (MultiH  (None, 144, 64)     66368       ['layer_normalization_19[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " add_18 (Add)                   (None, 144, 64)      0           ['multi_head_attention_9[0][0]', \n",
            "                                                                  'add_17[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_20 (LayerN  (None, 144, 64)     128         ['add_18[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_23 (Dense)               (None, 144, 128)     8320        ['layer_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_21 (Dropout)           (None, 144, 128)     0           ['dense_23[0][0]']               \n",
            "                                                                                                  \n",
            " dense_24 (Dense)               (None, 144, 64)      8256        ['dropout_21[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_22 (Dropout)           (None, 144, 64)      0           ['dense_24[0][0]']               \n",
            "                                                                                                  \n",
            " add_19 (Add)                   (None, 144, 64)      0           ['dropout_22[0][0]',             \n",
            "                                                                  'add_18[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_21 (LayerN  (None, 144, 64)     128         ['add_19[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_10 (Multi  (None, 144, 64)     66368       ['layer_normalization_21[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " add_20 (Add)                   (None, 144, 64)      0           ['multi_head_attention_10[0][0]',\n",
            "                                                                  'add_19[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_22 (LayerN  (None, 144, 64)     128         ['add_20[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_25 (Dense)               (None, 144, 128)     8320        ['layer_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_23 (Dropout)           (None, 144, 128)     0           ['dense_25[0][0]']               \n",
            "                                                                                                  \n",
            " dense_26 (Dense)               (None, 144, 64)      8256        ['dropout_23[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_24 (Dropout)           (None, 144, 64)      0           ['dense_26[0][0]']               \n",
            "                                                                                                  \n",
            " add_21 (Add)                   (None, 144, 64)      0           ['dropout_24[0][0]',             \n",
            "                                                                  'add_20[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_23 (LayerN  (None, 144, 64)     128         ['add_21[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_11 (Multi  (None, 144, 64)     66368       ['layer_normalization_23[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_22 (Add)                   (None, 144, 64)      0           ['multi_head_attention_11[0][0]',\n",
            "                                                                  'add_21[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_24 (LayerN  (None, 144, 64)     128         ['add_22[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_27 (Dense)               (None, 144, 128)     8320        ['layer_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_25 (Dropout)           (None, 144, 128)     0           ['dense_27[0][0]']               \n",
            "                                                                                                  \n",
            " dense_28 (Dense)               (None, 144, 64)      8256        ['dropout_25[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_26 (Dropout)           (None, 144, 64)      0           ['dense_28[0][0]']               \n",
            "                                                                                                  \n",
            " add_23 (Add)                   (None, 144, 64)      0           ['dropout_26[0][0]',             \n",
            "                                                                  'add_22[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_25 (LayerN  (None, 144, 64)     128         ['add_23[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_12 (Multi  (None, 144, 64)     66368       ['layer_normalization_25[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " add_24 (Add)                   (None, 144, 64)      0           ['multi_head_attention_12[0][0]',\n",
            "                                                                  'add_23[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_26 (LayerN  (None, 144, 64)     128         ['add_24[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_29 (Dense)               (None, 144, 128)     8320        ['layer_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_27 (Dropout)           (None, 144, 128)     0           ['dense_29[0][0]']               \n",
            "                                                                                                  \n",
            " dense_30 (Dense)               (None, 144, 64)      8256        ['dropout_27[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_28 (Dropout)           (None, 144, 64)      0           ['dense_30[0][0]']               \n",
            "                                                                                                  \n",
            " add_25 (Add)                   (None, 144, 64)      0           ['dropout_28[0][0]',             \n",
            "                                                                  'add_24[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_27 (LayerN  (None, 144, 64)     128         ['add_25[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_13 (Multi  (None, 144, 64)     66368       ['layer_normalization_27[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " add_26 (Add)                   (None, 144, 64)      0           ['multi_head_attention_13[0][0]',\n",
            "                                                                  'add_25[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_28 (LayerN  (None, 144, 64)     128         ['add_26[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_31 (Dense)               (None, 144, 128)     8320        ['layer_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_29 (Dropout)           (None, 144, 128)     0           ['dense_31[0][0]']               \n",
            "                                                                                                  \n",
            " dense_32 (Dense)               (None, 144, 64)      8256        ['dropout_29[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_30 (Dropout)           (None, 144, 64)      0           ['dense_32[0][0]']               \n",
            "                                                                                                  \n",
            " add_27 (Add)                   (None, 144, 64)      0           ['dropout_30[0][0]',             \n",
            "                                                                  'add_26[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_29 (LayerN  (None, 144, 64)     128         ['add_27[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_14 (Multi  (None, 144, 64)     66368       ['layer_normalization_29[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_28 (Add)                   (None, 144, 64)      0           ['multi_head_attention_14[0][0]',\n",
            "                                                                  'add_27[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_30 (LayerN  (None, 144, 64)     128         ['add_28[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_33 (Dense)               (None, 144, 128)     8320        ['layer_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_31 (Dropout)           (None, 144, 128)     0           ['dense_33[0][0]']               \n",
            "                                                                                                  \n",
            " dense_34 (Dense)               (None, 144, 64)      8256        ['dropout_31[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_32 (Dropout)           (None, 144, 64)      0           ['dense_34[0][0]']               \n",
            "                                                                                                  \n",
            " add_29 (Add)                   (None, 144, 64)      0           ['dropout_32[0][0]',             \n",
            "                                                                  'add_28[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_31 (LayerN  (None, 144, 64)     128         ['add_29[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_15 (Multi  (None, 144, 64)     66368       ['layer_normalization_31[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " add_30 (Add)                   (None, 144, 64)      0           ['multi_head_attention_15[0][0]',\n",
            "                                                                  'add_29[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_32 (LayerN  (None, 144, 64)     128         ['add_30[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_35 (Dense)               (None, 144, 128)     8320        ['layer_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_33 (Dropout)           (None, 144, 128)     0           ['dense_35[0][0]']               \n",
            "                                                                                                  \n",
            " dense_36 (Dense)               (None, 144, 64)      8256        ['dropout_33[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_34 (Dropout)           (None, 144, 64)      0           ['dense_36[0][0]']               \n",
            "                                                                                                  \n",
            " add_31 (Add)                   (None, 144, 64)      0           ['dropout_34[0][0]',             \n",
            "                                                                  'add_30[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_33 (LayerN  (None, 144, 64)     128         ['add_31[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 9216)         0           ['layer_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_35 (Dropout)           (None, 9216)         0           ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_37 (Dense)               (None, 2048)         18876416    ['dropout_35[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_36 (Dropout)           (None, 2048)         0           ['dense_37[0][0]']               \n",
            "                                                                                                  \n",
            " dense_38 (Dense)               (None, 1024)         2098176     ['dropout_36[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 1024)         0           ['dense_38[0][0]']               \n",
            "                                                                                                  \n",
            " dense_39 (Dense)               (None, 2)            2050        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 21,658,569\n",
            "Trainable params: 21,658,562\n",
            "Non-trainable params: 7\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "vit_classifier.summary()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "mount_file_id": "12AEoKfQgucgd2X7J2xLjHIMOB1uvUINv",
      "authorship_tag": "ABX9TyMwHw5xzcZmeZDyyumHkkzN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}